{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"feature_eng\": {\n",
    "\n",
    "    },\n",
    "    \"modeling\": {\n",
    "        \"dummy\": False,\n",
    "        \"linear_reg\": False,\n",
    "        \"svr\": False,\n",
    "        \"tree\": False,\n",
    "        \"forest\": False,\n",
    "        \"xgboost\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Dependency import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import set_config\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import shap\n",
    "\n",
    "# plt.style.use([\"dark_background\"])\n",
    "plt.style.use([\"default\"])\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_features_name(dataset, split_by_unique_count=True, split_count=10):\n",
    "    features_name = dataset.select_dtypes([\"object\", \"bool\"]).columns\n",
    "    if split_by_unique_count:\n",
    "        less_uniques = [feature_name for feature_name in features_name if dataset[feature_name].nunique() <= split_count]\n",
    "        lot_uniques = features_name.difference(less_uniques).tolist()\n",
    "        return (less_uniques, lot_uniques)\n",
    "    else:\n",
    "        return features_name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_features_name(dataset):\n",
    "    features_name = dataset.select_dtypes([\"int64\", \"float64\"]).columns.values.tolist()\n",
    "    return features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationResult():\n",
    "    def __init__(self, gridsearch, target, X_train, y_train, X_test, y_test, num_cols, cat_less_unique_cols, cat_lot_unique_cols):\n",
    "        self.__gs = gridsearch\n",
    "        self.__X_train = X_train\n",
    "        self.__y_train = y_train\n",
    "        self.__X_test = X_test\n",
    "        self.__y_test = y_test\n",
    "        self.__target = target\n",
    "        self.__score = self.__gs.score(self.__X_test, self.__y_test)\n",
    "        self.__ratio = np.abs((self.__score * 100) / self.__y_test.mean())\n",
    "        self.__num_cols = num_cols\n",
    "        self.__cat_less_unique_cols = cat_less_unique_cols\n",
    "        self.__cat_lot_unique_cols = cat_lot_unique_cols\n",
    "        self.__cat_less_unique_cols_preproc = self.__gs.best_estimator_[\"transforms\"].transformers_[1][1][\"one_hot_encoder\"].get_feature_names_out(self.__cat_less_unique_cols).tolist()\n",
    "        self.__cat_lot_unique_cols_preproc = self.__gs.best_estimator_[\"transforms\"].transformers_[2][1][\"one_hot_encoder\"].get_feature_names_out(self.__cat_lot_unique_cols).tolist()\n",
    "\n",
    "        self.best_model = self.__gs.best_estimator_.named_steps[\"model\"]\n",
    "        self.feature_names = self.__num_cols + self.__cat_less_unique_cols_preproc + self.__cat_lot_unique_cols_preproc\n",
    "        self.X_train_transform = pd.DataFrame(data=self.__gs.best_estimator_.named_steps[\"transforms\"].transform(self.__X_train).toarray(), columns=self.feature_names)\n",
    "        self.X_test_transform = pd.DataFrame(data=self.__gs.best_estimator_.named_steps[\"transforms\"].transform(self.__X_test).toarray(), columns=self.feature_names)\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(f\"RMSE: {-self.__score:.4}\")\n",
    "        print(f\"Target mean value: {self.__target.mean():.4}\")\n",
    "        print(f\"Ratio: {self.__ratio:.4}%\")\n",
    "        print(f\"best_params: {self.__gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, grid_params, dataset, target, scoring=\"neg_root_mean_squared_error\"):\n",
    "    set_config(display=\"diagram\") # display=\"text\" -> for textual output\n",
    "\n",
    "    ### DATASET PREPARATION ###\n",
    "\n",
    "    y = dataset[target].copy()\n",
    "    X = dataset.drop(columns=[target]).copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "    categorical_cols_less_unique, categorical_cols_lot_unique = get_categorical_features_name(X_train)\n",
    "    numerical_cols = get_numerical_features_name(X_train)\n",
    "\n",
    "    ### PIPELINE CONSTRUCTION ###\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"simple_imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "    ])\n",
    "\n",
    "    cat_less_unique_pipe = Pipeline(steps=[\n",
    "        (\"simple_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    cat_lot_unique_pipe = Pipeline(steps=[\n",
    "        (\"simple_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num_pipe\", num_pipe, numerical_cols),\n",
    "        (\"cat_less_unique_pipe\", cat_less_unique_pipe, categorical_cols_less_unique),\n",
    "        (\"cat_lot_unique_pipe\", cat_lot_unique_pipe, categorical_cols_lot_unique)\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"transforms\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    ### GRIDSEARCH DECLARATION AND FITTING ###\n",
    "\n",
    "    gs = GridSearchCV(pipeline, grid_params, scoring=scoring, refit=True)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    return EvaluationResult(gs, y, X_train, y_train, X_test, y_test, numerical_cols, categorical_cols_less_unique, categorical_cols_lot_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data-cleaned.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = \"SiteEnergyUse(kBtu)\"\n",
    "target_2 = \"GHGEmissionsIntensity(kgCO2e/ft2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = data[target]\n",
    "# X = data.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_full, X_test_full, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_cols_less_unique = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n",
    "# categorical_cols_lot_unique = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() >= 10 and X_train_full[cname].dtype == \"object\"]\n",
    "# numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_cols = categorical_cols_lot_unique + categorical_cols_less_unique + numerical_cols\n",
    "# X_train = X_train_full[my_cols].copy()\n",
    "# X_test = X_test_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_MI = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_MI[\"DefaultData\"] = X_MI[\"DefaultData\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_MI_NUM = X_MI.select_dtypes([\"int64\", \"float64\"]).columns\n",
    "# X_MI_CAT = X_MI.select_dtypes([\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete_features = X_MI.dtypes == object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impt_num = SimpleImputer(strategy=\"mean\")\n",
    "# X_MI[X_MI_NUM] = pd.DataFrame(impt_num.fit_transform(X_MI[X_MI_NUM]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impt_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "# X_MI[X_MI_CAT] = pd.DataFrame(impt_cat.fit_transform(X_MI[X_MI_CAT]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in X_MI_CAT:\n",
    "#     X_MI[col], _ = X_MI[col].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = make_mi_scores(X_MI, y, discrete_features).sort_values(ascending=False)\n",
    "# scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mi_scores(scores):\n",
    "#     plt.figure(dpi=100, figsize=(8, 5))\n",
    "#     scores = scores.sort_values(ascending=True)\n",
    "#     width = np.arange(len(scores))\n",
    "#     ticks = list(scores.index)\n",
    "#     plt.barh(width, scores)\n",
    "#     plt.yticks(width, ticks)\n",
    "#     plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "# plot_mi_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.1 DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"modeling\"][\"dummy\"]:\n",
    "\n",
    "    model = DummyRegressor()\n",
    "\n",
    "    grid_params = [\n",
    "        {\n",
    "            \"model__strategy\": [\"mean\", \"median\"]\n",
    "        },\n",
    "        {\n",
    "            \"model__strategy\": [\"quantile\"],\n",
    "            \"model__quantile\": np.arange(0, 1.1, 0.1),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score, ratio, gs, pipeline = evaluate(model, grid_params)\n",
    "\n",
    "    print(f\"RMSE: {score:n} / RATIO: {ratio:.4}%\")\n",
    "    print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.2 LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"modeling\"][\"linear_reg\"]:\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    grid_params = [\n",
    "        {\n",
    "            \"model__fit_intercept\": [True]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score, ratio, gs, pipeline = evaluate(model, grid_params)\n",
    "\n",
    "    print(f\"RMSE: {score:n} / RATIO: {ratio:.4}%\")\n",
    "    print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.3 SupportVectorRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"modeling\"][\"svr\"]:\n",
    "\n",
    "    model = SVR()\n",
    "\n",
    "    grid_params = [\n",
    "        {\n",
    "            \"model__kernel\": [\"rbf\"],\n",
    "            \"model__degree\": [3],\n",
    "            \"model__gamma\": [\"scale\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score, ratio, gs, pipeline = evaluate(model, grid_params)\n",
    "\n",
    "    print(f\"RMSE: {score:n} / RATIO: {ratio:.4}%\")\n",
    "    print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.4 DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"modeling\"][\"tree\"]:\n",
    "\n",
    "    model = DecisionTreeRegressor()\n",
    "\n",
    "    grid_params = [\n",
    "        {\n",
    "            \"model__random_state\": [1],\n",
    "            \"model__max_depth\": [2, 3, 4],\n",
    "            \"model__min_samples_leaf\": range(1, 11, 1),\n",
    "            \"model__criterion\": [\"mse\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score, ratio, gs, pipeline = evaluate(model, grid_params)\n",
    "\n",
    "    print(f\"RMSE: {score:n} / RATIO: {ratio:.4}%\")\n",
    "    print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.5 RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"modeling\"][\"forest\"]:\n",
    "\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    grid_params = [\n",
    "        {\n",
    "            \"model__random_state\": [1],\n",
    "            \"model__n_estimators\": [25],\n",
    "            \"model__min_samples_leaf\": [1],\n",
    "            \"model__criterion\": [\"mse\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score, ratio, gs, pipeline = evaluate(model, grid_params)\n",
    "\n",
    "    print(f\"RMSE: {score:n} / RATIO: {ratio:.4}%\")\n",
    "    print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.6 XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"modeling\"][\"xgboost\"]:\n",
    "\n",
    "    model = XGBRegressor()\n",
    "\n",
    "    grid_params = [\n",
    "        {\n",
    "            \"model__random_state\": [1],\n",
    "            \"model__max_depth\": [3],\n",
    "            \"model__n_estimators\": [100],\n",
    "            \"model__learning_rate\": [0.1],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score, ratio, pipeline, gs, X_train, y_train, X_test, y_test, categorical_cols_less_unique, categorical_cols_lot_unique = evaluate(model, grid_params, data, target_1)\n",
    "\n",
    "    print(f\"RMSE: {-score:.4}\")\n",
    "    print(f\"Target mean value: {y_train.mean():.4}\")\n",
    "    print(f\"Ratio: {ratio:.4}%\")\n",
    "    print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.7 Permutation, PDP, SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor()\n",
    "\n",
    "grid_params = [\n",
    "    {\n",
    "        \"model__random_state\": [1],\n",
    "        \"model__max_depth\": [3],\n",
    "        \"model__n_estimators\": [100],\n",
    "        \"model__learning_rate\": [0.1],\n",
    "    }\n",
    "]\n",
    "\n",
    "score, ratio, target, gs, best_model, feature_names = evaluate(model, grid_params, data, target_1)\n",
    "\n",
    "print(f\"RMSE: {-score:.4}\")\n",
    "print(f\"Target mean value: {target.mean():.4}\")\n",
    "print(f\"Ratio: {ratio:.4}%\")\n",
    "print(f\"best_params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.6.3 Feature explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(eval_result.best_model, data=eval_result.X_train_transform)\n",
    "shap_values = explainer(eval_result.X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.force_plot(explainer.expected_value, shap_values.values) # takes too much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,\"Address_14027 lake city way ne\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6 ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
